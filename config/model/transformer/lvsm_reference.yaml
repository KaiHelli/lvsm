d_model: 768
d_k: 64
d_v: 64
num_heads: 16
d_ff: 4096
dropout_p: 0.0
num_encoder_layers: 24
num_decoder_layers: 0
bias: false
activation: gelu
pre_norm: true
qk_norm: true
qk_exp_seq_len: "${calc_exp_seq_len:${dataset.image_shape},${lvsm_cfg.patch_size},${dataset.view_sampler.num_context_views},${dataset.view_sampler.num_target_views},${oc.select:lvsm_cfg.vae_cfg}}"
sdpa_kernel: "flex-attention" # One of: ["flex-attention", "torch-sdpa", "naive", "auto"]