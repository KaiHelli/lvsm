d_model: 768
d_k: 64
d_v: 64
num_heads: 16
d_ff: 4096
dropout_p: 0.0
num_encoder_layers: 6
num_decoder_layers: 0
bias: false
activation: gelu
pre_norm: true
qk_norm: true
qk_exp_seq_len: "${calc_exp_seq_len:${dataset.image_shape},${model.lvsm.patch_size},${dataset.view_sampler.num_context_views},${dataset.view_sampler.num_target_views}}"
sdpa_kernel: "flex-attention" # One of: ["flex-attention", "torch-sdpa", "naive", "auto"]