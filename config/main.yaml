defaults:
  - dataset: re10k
  - optional dataset/view_sampler_dataset_specific_config: ${dataset/view_sampler}_${dataset}
  - model/lvsm@model.lvsm
  - loss: [mse, lpips]

wandb:
  project: lvsm
  entity: tum-edu
  name: test
  mode: online
  id: null

mode: train

dataset:
  overfit_to_scene: null #48eb62d05fc8104c

data_loader:
  # Avoid having to spin up new processes to print out visualizations.
  train:
    num_workers: 10
    persistent_workers: true
    batch_size: 3 # was 4
    seed: 1234
  test:
    num_workers: 4
    persistent_workers: false
    batch_size: 1
    seed: 2345
  val:
    num_workers: 1
    persistent_workers: true
    batch_size: 1
    seed: 3456
#  shims:
#    calculate_rays: true

optimizer:
  lr: 4e-4
  initial_lr: 1e-6
  min_lr: 1e-6
  warm_up_steps: 2500
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 0.05

checkpointing:
  load: outputs/**/**/checkpoints/*.ckpt
  every_n_train_steps: 100  # 5000
  save_top_k: 10
  pretrained_model: null
  resume: true

train:
  depth_mode: null
  extended_visualization: false
  print_log_every_n_steps: 1
  val_every_n_batches: 50
  vis_every_n_validations: 40       # 50 * 40 -> every 2000 batches

test:
  output_path: outputs/test
  compute_scores: false
  eval_time_skip_steps: 0
  save_image: true
  save_video: false

seed: 111123

trainer:
  max_steps: -1
  check_val_every_n_epoch: null
  val_check_interval: ${train.val_every_n_batches}
  gradient_clip_val: 0.5 # try to use 5.0
  accumulate_grad_batches: 4
  num_sanity_val_steps: 2
  num_nodes: 1
  precision: "bf16-mixed"
  log_every_n_steps: 10

output_dir: null
