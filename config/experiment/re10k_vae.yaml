# @package _global_

defaults:
  - override /dataset: re10k
  - override /loss: [mse]
  - override /model/vae@lvsm_cfg.vae_cfg: sd35
  - override /dataset/view_sampler: boundedv2

lvsm_cfg:
  transformer_cfg:
    sdpa_kernel: torch-sdpa  # Due to bug after applying torch.compile on flex-attention

wandb:
  name: re10k
  tags: [re10k, 192x192]

data_loader:
  train:
    batch_size: 64

trainer:
  max_steps: 100_001
  accumulate_grad_batches: 1
  log_every_n_steps: 1

dataset: 
  image_shape: [192, 192]
  roots: [datasets/re10k]
  near: 1.
  far: 100.
  baseline_scale_bounds: false
  make_baseline_1: false
  sort_target_index: true
  sort_context_index: true 
  view_sampler:
    target_views_replace_sample: false
    num_context_views: 2  # Only relevant for the test stage, as we are sampling randomly below
    num_target_views: 6
    random_num_context_views: True
    match_random_num_context_views_per_batch: True
    min_context_views: 2
    max_context_views: 8
    max_context_views_warm_up_steps: 60_000
    extra_views_sampling_strategy: farthest_point

test:
  eval_time_skip_steps: 5
  compute_scores: true
