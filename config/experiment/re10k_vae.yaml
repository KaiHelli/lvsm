# @package _global_

defaults:
  - override /dataset: re10k
  - override /loss: [mse]
  - override /model/vae@lvsm_cfg.vae_cfg: sd35

lvsm_cfg:
  transformer_cfg:
    sdpa_kernel: torch-sdpa  # Due to bug after applying torch.compile on flex-attention

wandb:
  name: re10k
  tags: [re10k, 192x192]

data_loader:
  train:
    batch_size: 64

trainer:
  max_steps: 100_001
  accumulate_grad_batches: 1
  log_every_n_steps: 1

dataset: 
  image_shape: [192, 192]
  roots: [datasets/re10k]
  near: 1.
  far: 100.
  baseline_scale_bounds: false
  make_baseline_1: false

test:
  eval_time_skip_steps: 5
  compute_scores: true
