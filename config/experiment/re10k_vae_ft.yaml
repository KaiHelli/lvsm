# @package _global_

defaults:
  - override /dataset: re10k
  - override /loss: vis_loss
  - override /model/vae@lvsm_cfg.vae_cfg: sd35
  - override /dataset/view_sampler: boundedv2

lvsm_cfg:
  transformer_cfg:
    sdpa_kernel: torch-sdpa  # Due to bug after applying torch.compile on flex-attention

wandb:
  name: re10k
  tags: [re10k, 192x192]

data_loader:
  train:
    batch_size: 4

trainer:
  max_steps: 20_000
  accumulate_grad_batches: 16
  log_every_n_steps: 1

dataset: 
  image_shape: [192, 192]
  roots: [datasets/re10k]
  near: 1.
  far: 100.
  baseline_scale_bounds: false
  make_baseline_1: false
  sort_target_index: true
  sort_context_index: true 
  view_sampler:
    target_views_replace_sample: false
    num_context_views: 2  # Only relevant for the test stage, as we are sampling randomly below
    num_target_views: 6
    random_num_context_views: True
    match_random_num_context_views_per_batch: True
    min_context_views: 2
    max_context_views: 8
    max_context_views_warm_up_steps: 10_000
    extra_views_sampling_strategy: farthest_point

train:
  decode_vae_latents: true
  finetune_vae_decoder: 
    layer_config:
      Linear:
        weight:
          lora_rank: 8
          lora_dropout_p: 0.0
          lora_alpha: 2.0
      Conv2d:
        weight:
          lora_rank: 8
          lora_dropout_p: 0.0
          lora_alpha: 2.0

test:
  eval_time_skip_steps: 5
  compute_scores: true

checkpointing:
  resume: false